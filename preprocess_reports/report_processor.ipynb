{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION - Edit this section for each report\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__ if '__file__' in globals() else '.'))))\n",
    "REPORTS_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "\n",
    "report_config = {\n",
    "    \"report_name\": \"ANNUAL_REPORT_2023\",  # Options: ANNUAL_REPORT_2023, ART7_REPORT_2022, CAMBODIA_CMR_2023, CAMBODIA_MINES_2023, IWP_2023\n",
    "    \n",
    "    # Model configuration\n",
    "    \"models\": {\n",
    "        \"mistral:instruct\": \"mistral-instruct\",\n",
    "        \"vicuna\": \"vicuna\", \n",
    "        \"llama3\": \"llama3\"\n",
    "    },\n",
    "    \n",
    "    # Processing settings\n",
    "    \"request_timeout\": 2000,\n",
    "    \"use_groq\": False,  # Set True for Groq API models\n",
    "    \"use_azure_openai\": False  # Set True for Azure OpenAI models\n",
    "}\n",
    "\n",
    "# Predefined configurations with generic, relative paths\n",
    "predefined_configs = {\n",
    "    \"ANNUAL_REPORT_2023\": {\n",
    "        \"input_csv\": os.path.join(REPORTS_DIR, \"ANNUAL-REPORT-2023-output.csv\"),\n",
    "        \"output_csv\": os.path.join(REPORTS_DIR, \"ANNUAL-REPORT-2023-output.csv\")\n",
    "    },\n",
    "    \"ART7_REPORT_2022\": {\n",
    "        \"input_csv\": os.path.join(REPORTS_DIR, \"2023-Cambodia-Art7Report-for2022.csv\"),\n",
    "        \"output_csv\": os.path.join(REPORTS_DIR, \"2023-Cambodia-Art7Report-for2022-output.csv\")\n",
    "    },\n",
    "    \"CAMBODIA_CMR_2023\": {\n",
    "        \"input_csv\": os.path.join(REPORTS_DIR, \"CAMBODIA_CLEARING_CMR_2023.csv\"),\n",
    "        \"output_csv\": os.path.join(REPORTS_DIR, \"CAMBODIA_CLEARING_CMR_2023-output.csv\")\n",
    "    },\n",
    "    \"CAMBODIA_MINES_2023\": {\n",
    "        \"input_csv\": os.path.join(REPORTS_DIR, \"Cambodia_Clearing_the_Mines_2023.csv\"),\n",
    "        \"output_csv\": os.path.join(REPORTS_DIR, \"Cambodia_Clearing_the_Mines_2023-output.csv\")\n",
    "    },\n",
    "    \"IWP_2023\": {\n",
    "        \"input_csv\": os.path.join(REPORTS_DIR, \"IWP-2023.csv\"),\n",
    "        \"output_csv\": os.path.join(REPORTS_DIR, \"IWP-2023-output.csv\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply predefined config if specified\n",
    "if report_config[\"report_name\"] in predefined_configs:\n",
    "    config = predefined_configs[report_config[\"report_name\"]]\n",
    "    report_config.update(config)\n",
    "\n",
    "print(f\"Processing report: {report_config['report_name']}\")\n",
    "print(f\"Input CSV: {report_config.get('input_csv', 'Not specified')}\")\n",
    "print(f\"Output CSV: {report_config.get('output_csv', 'Not specified')}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Optional imports (uncomment if needed)\n",
    "# import openparse  # For PDF parsing\n",
    "# from openai import AzureOpenAI  # For Azure OpenAI\n",
    "# from groq import Groq  # For Groq API\n",
    "\n",
    "print(\"Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "if \"input_csv\" not in report_config:\n",
    "    raise ValueError(\"input_csv must be specified in report_config\")\n",
    "\n",
    "prompts = pd.read_csv(report_config[\"input_csv\"])\n",
    "print(f\"Loaded {len(prompts)} prompts from {report_config['input_csv']}\")\n",
    "print(f\"Columns: {list(prompts.columns)}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = os.path.dirname(report_config[\"output_csv\"])\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL PROCESSING\n",
    "def process_with_models(prompts_df, models_dict, config):\n",
    "    \"\"\"\n",
    "    Process prompts with specified models\n",
    "    \"\"\"\n",
    "    results_df = prompts_df.copy()\n",
    "    \n",
    "    for model_name, column_name in models_dict.items():\n",
    "        print(f\"\\n=== Processing with model: {model_name} ===\")\n",
    "        \n",
    "        # Initialize the appropriate LLM\n",
    "        if config.get(\"use_groq\", False):\n",
    "            # Groq initialization (uncomment and configure as needed)\n",
    "            # client = Groq(api_key=\"your_groq_api_key\")\n",
    "            # llm = client  # Configure appropriately\n",
    "            raise NotImplementedError(\"Groq configuration needed\")\n",
    "        elif config.get(\"use_azure_openai\", False):\n",
    "            # Azure OpenAI initialization (uncomment and configure as needed)\n",
    "            # client = AzureOpenAI(azure_endpoint=\"your_endpoint\", api_key=\"your_key\", api_version=\"version\")\n",
    "            # llm = client  # Configure appropriately\n",
    "            raise NotImplementedError(\"Azure OpenAI configuration needed\")\n",
    "        else:\n",
    "            # Default: Ollama\n",
    "            llm = Ollama(model=model_name, request_timeout=config.get(\"request_timeout\", 2000))\n",
    "        \n",
    "        # Process each prompt\n",
    "        try:\n",
    "            for i, row in results_df.iterrows():\n",
    "                # Check if already processed\n",
    "                if column_name in results_df.columns and pd.notna(row[column_name]):\n",
    "                    print(f\"Skipping prompt {i + 1} for model {model_name} (already processed)\")\n",
    "                    continue\n",
    "                \n",
    "                prompt = row['Prompts']\n",
    "                print(f\"Processing prompt {i + 1}/{len(results_df)} with {model_name}\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Generate response based on model type\n",
    "                if config.get(\"use_groq\", False) or config.get(\"use_azure_openai\", False):\n",
    "                    # For API-based models (implement as needed)\n",
    "                    response_text = \"API response not implemented\"\n",
    "                else:\n",
    "                    # For Ollama\n",
    "                    response = llm.complete(prompt)\n",
    "                    response_text = str(response)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "                \n",
    "                # Update DataFrame\n",
    "                results_df.at[i, column_name] = response_text\n",
    "                results_df.at[i, f'Processing Time ({column_name})'] = processing_time\n",
    "                \n",
    "                # Save progress\n",
    "                results_df.to_csv(config[\"output_csv\"], index=False)\n",
    "                \n",
    "                print(f\"Completed in {processing_time:.2f}s\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with model {model_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        finally:\n",
    "            # Clear GPU memory if using CUDA\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Finished processing with {model_name}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run processing\n",
    "results = process_with_models(prompts, report_config[\"models\"], report_config)\n",
    "print(f\"\\n=== Processing Complete ===\")\n",
    "print(f\"Results saved to: {report_config['output_csv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY\n",
    "print(\"\\n=== Processing Summary ===\")\n",
    "print(f\"Report: {report_config['report_name']}\")\n",
    "print(f\"Total prompts: {len(results)}\")\n",
    "print(f\"Models processed: {list(report_config['models'].keys())}\")\n",
    "print(f\"Output file: {report_config['output_csv']}\")\n",
    "\n",
    "# Show completion status for each model\n",
    "for model_name, column_name in report_config[\"models\"].items():\n",
    "    if column_name in results.columns:\n",
    "        completed = results[column_name].notna().sum()\n",
    "        print(f\"  {model_name}: {completed}/{len(results)} prompts completed\")\n",
    "    else:\n",
    "        print(f\"  {model_name}: Column not found\")\n",
    "\n",
    "print(\"\\n=== Done ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}