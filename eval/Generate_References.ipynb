{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685f9819-266d-498e-9f6a-343339030511",
   "metadata": {},
   "source": [
    "#### Generate references with GPT4o and Llama3-70b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b65f3c-b821-4108-a140-e19a90cdf297",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490499c-2467-44a4-91bc-1e6855023608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Get the base directory dynamically \n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__ if '__file__' in globals() else '.')))\n",
    "EVAL_DIR = os.path.join(BASE_DIR, \"eval\")\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "\n",
    "# Initialize the OpenAI client with environment variables for security\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
    "\n",
    "if not azure_api_key:\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY environment variable is required\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=azure_api_key,\n",
    "    api_version=azure_api_version\n",
    ")\n",
    "\n",
    "print(\"Azure OpenAI client initialized successfully\")\n",
    "print(\"\\nEnvironment Variables Needed:\")\n",
    "print(\"- AZURE_OPENAI_API_KEY: Your Azure OpenAI API key (required)\")\n",
    "print(\"- AZURE_OPENAI_ENDPOINT: Your Azure OpenAI endpoint (optional, defaults to cmac-openai-default)\")\n",
    "print(\"- AZURE_OPENAI_API_VERSION: Your Azure OpenAI API version (optional, defaults to 2024-02-15-preview)\")\n",
    "\n",
    "# Read the random_prompts.csv file with dynamic path\n",
    "csv_file_path = os.path.join(EVAL_DIR, 'updated_random_prompts.csv')\n",
    "random_prompts = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Add new columns for the generated text and processing time\n",
    "random_prompts['GPT-4o'] = None\n",
    "random_prompts['Processing Time (GPT-4o)'] = None\n",
    "\n",
    "try:\n",
    "    # Iterate through each prompt\n",
    "    for i, row in random_prompts.iterrows():      \n",
    "        prompt = row['Combined Text']\n",
    "        print(f\"Processing prompt {i + 1} out of {len(random_prompts)}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-default\",  # model = \"deployment_name\".\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        # Update the DataFrame with the response and processing time\n",
    "        random_prompts.at[i, 'GPT-4o'] = response.choices[0].message.content\n",
    "        random_prompts.at[i, 'Processing Time (GPT-4o)'] = processing_time\n",
    "\n",
    "        print(f\"Query LLM processing time: {processing_time}\")\n",
    "        print(f\"Model's Answer for prompt {i + 1}:\\n{response.choices[0].message.content}\\n\")\n",
    "\n",
    "        # Save the updated DataFrame back to a CSV file after each prompt\n",
    "        random_prompts.to_csv('updated_random_prompts.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f9adb-6f23-44c5-8b94-d2bbffd5f387",
   "metadata": {},
   "source": [
    "####  Llama3-70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9a522-1edd-4d71-88e0-64ab3d17fe84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq(\n",
    "    api_key=\"key\"\n",
    ")\n",
    "\n",
    "# Add new columns for the generated text and processing time \n",
    "random_prompts['llama3-70b'] = None\n",
    "random_prompts['Processing Time (llama3-70b)'] = None\n",
    "\n",
    "try:\n",
    "    # Iterate through each prompt\n",
    "    for i, row in random_prompts.iterrows():\n",
    "        prompt = row['Combined Text']\n",
    "        print(f\"Processing prompt {i + 1} out of {len(random_prompts)}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        llm = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        # Update the DataFrame with the response and processing time\n",
    "        random_prompts.at[i, 'llama3-70b'] = llm.choices[0].message.content\n",
    "        random_prompts.at[i, 'Processing Time (llama3-70b)'] = processing_time\n",
    "\n",
    "        print(f\"Query LLM processing time: {processing_time}\")\n",
    "        print(f\"Model's Answer for prompt {i + 1}:\\n{llm.choices[0].message.content}\\n\")\n",
    "\n",
    "        # Save the updated DataFrame back to a CSV file after each prompt\n",
    "        random_prompts.to_csv('updated_random_prompts.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b5f7d-8bf4-4d71-8319-c1be8c605e62",
   "metadata": {},
   "source": [
    "#### 100(150) prompts triples (vicuna, llama3,mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1170a0-8e41-4f41-9d66-34b86085e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering prompts during annotation, throw bad ones, add new ones\n",
    "# random_prompts.csv\n",
    "import pandas as pd\n",
    "# Read the random_prompts.csv file\n",
    "csv_file_path = 'updated_random_prompts.csv'\n",
    "random_prompts = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b640a7-ec4e-491a-8779-b2fc8c5129f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = Ollama(model=\"mistral:instruct\")\n",
    "\n",
    "try:\n",
    "    for i, row in random_prompts.iloc[128:].iterrows():\n",
    "        prompt = row['Combined Text'] \n",
    "        print(f\"Processing prompt {i + 1} out of {len(random_prompts)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = llm.complete(prompt)\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Update the DataFrame with the response and processing time\n",
    "        random_prompts.at[i, 'mistral:instruct'] = response\n",
    "        random_prompts.at[i, 'Processing Time (mistral)'] = processing_time\n",
    "        \n",
    "        print(f\"Query LLM processing time: {processing_time}\")\n",
    "        print(f\"Model's Answer for prompt {i + 1}:\\n{response}\\n\")\n",
    "        \n",
    "        # Save the updated DataFrame back to a CSV file after each prompt\n",
    "        random_prompts.to_csv('updated_random_prompts.csv', index=False)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeef6a5-a75c-4e75-b1fa-e365e6617707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = Ollama(model=\"vicuna\")\n",
    "\n",
    "# Add new columns for the generated text and processing time\n",
    "random_prompts['vicuna'] = None\n",
    "random_prompts['Processing Time (vicuna) '] = None\n",
    "\n",
    "try:\n",
    "    for i, row in random_prompts.iterrows():\n",
    "        prompt = row['Combined Text'] \n",
    "        print(f\"Processing prompt {i + 1} out of {len(random_prompts)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = llm.complete(prompt)\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Update the DataFrame with the response and processing time\n",
    "        random_prompts.at[i, 'vicuna'] = response\n",
    "        random_prompts.at[i, 'Processing Time (vicuna)'] = processing_time\n",
    "        \n",
    "        print(f\"Query LLM processing time: {processing_time}\")\n",
    "        print(f\"Model's Answer for prompt {i + 1}:\\n{response}\\n\")\n",
    "        \n",
    "        # Save the updated DataFrame back to a CSV file after each prompt\n",
    "        random_prompts.to_csv('updated_random_prompts.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bf50e-deb5-41d1-b534-163743f74599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Add new columns for the generated text and processing time\n",
    "random_prompts['llama3'] = None\n",
    "random_prompts['Processing Time (llama3) '] = None\n",
    "\n",
    "try:\n",
    "    for i, row in random_prompts.iterrows():\n",
    "        prompt = row['Combined Text'] \n",
    "        print(f\"Processing prompt {i + 1} out of {len(random_prompts)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = llm.complete(prompt)\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Update the DataFrame with the response and processing time\n",
    "        random_prompts.at[i, 'llama3'] = response\n",
    "        random_prompts.at[i, 'Processing Time (llama3)'] = processing_time\n",
    "        \n",
    "        print(f\"Query LLM processing time: {processing_time}\")\n",
    "        print(f\"Model's Answer for prompt {i + 1}:\\n{response}\\n\")\n",
    "        \n",
    "        # Save the updated DataFrame back to a CSV file after each prompt\n",
    "        random_prompts.to_csv('updated_random_prompts.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}