{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc872d5-ac00-4bcd-9eb3-0ee2753eda99",
   "metadata": {},
   "source": [
    "## Accuracy metrics ( Bleu, Rouge, Meteor, BertScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635afe4f-5723-4712-9efb-1bba46067ae8",
   "metadata": {},
   "source": [
    "### Concatenate triples from six reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ded420-2a13-4c2c-b48e-de6dd2dd3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION \n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load  # Import the evaluate library for METEOR\n",
    "from bert_score import score\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Get the base directory dynamically \n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__ if '__file__' in globals() else '.')))\n",
    "EVAL_DIR = os.path.join(BASE_DIR, \"eval\")\n",
    "TRIPLE_DIR = os.path.join(BASE_DIR, \"Triple_preprocessing\")\n",
    "REPORTS_DIR = os.path.join(BASE_DIR, \"reports\")\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Triple preprocessing directory: {TRIPLE_DIR}\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e732638-beac-462d-86b2-29bbbc5e0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate triples from six reports for vicuna, llama, mistral, GPT-40, llama3-70b\n",
    "df1 = pd.read_csv(os.path.join(TRIPLE_DIR, \"2023-Cambodia-Art7Report-for2022\", \"cleaned_output.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(TRIPLE_DIR, \"Annual report 2023\", \"cleaned_output.csv\"))\n",
    "df3 = pd.read_csv(os.path.join(TRIPLE_DIR, \"CAMBODIA_CLEARING_CMR_2023\", \"cleaned_output.csv\"))\n",
    "df4 = pd.read_csv(os.path.join(TRIPLE_DIR, \"Cambodia_Clearing_the_Mines_2023\", \"cleaned_output.csv\"))\n",
    "df5 = pd.read_csv(os.path.join(TRIPLE_DIR, \"IWP-2023\", \"cleaned_output.csv\"))\n",
    "df6 = pd.read_csv(os.path.join(TRIPLE_DIR, \"IWP-2024\", \"cleaned_output.csv\"))\n",
    "\n",
    "# Concatenate them along the rows\n",
    "df_all_triples = pd.concat([df1, df2, df3, df4, df5, df6], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33fadb-b4f6-4542-893a-08e610a306c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(TRIPLE_DIR, \"df_all_triples.csv\")\n",
    "df_all_triples.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe076111-4eea-4f77-8dd1-d25b777f3f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "annotation_df = pd.read_csv(os.path.join(EVAL_DIR, \"Annotations.csv\"))\n",
    "all_triples_A1_df = pd.read_csv(os.path.join(TRIPLE_DIR, \"All_triples_A1.csv\"))\n",
    "\n",
    "# Find rows in All_triples_A1 where the 'Prompts' column matches with the 'Prompts' column in Annotation\n",
    "evalute_A1_df = all_triples_A1_df[all_triples_A1_df['Prompts'].isin(annotation_df['Prompts'])]\n",
    "\n",
    "evalute_A1_df = evalute_A1_df.merge(annotation_df[['Prompts', 'Annotation']], on='Prompts', how='left')\n",
    "\n",
    "# Display the matching rows\n",
    "evalute_A1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e781db-70ae-4038-832e-353696eac0cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74082622-8f15-4f6c-8707-21154303dce6",
   "metadata": {},
   "source": [
    "### Preprocessing with stemming and lematizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a721c9-5f47-446b-bb12-161b6d175d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_df = pd.read_csv(os.path.join(REPORTS_DIR, \"RP_cleaned_output.csv\"))\n",
    "RS_df = pd.read_csv(os.path.join(REPORTS_DIR, \"RS_cleaned_output.csv\"))\n",
    "OP_df = pd.read_csv(os.path.join(REPORTS_DIR, \"OP_cleaned_output.csv\"))\n",
    "OS_df = pd.read_csv(os.path.join(REPORTS_DIR, \"OS_cleaned_output.csv\"))\n",
    "no_demo_df = pd.read_csv(os.path.join(BASE_DIR, \"no_demo_cleaned.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff872f-8783-4af6-8610-2adeaf13dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_demo_df['llama3-70b'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3beb4ea-3831-463a-b27c-46d7d0088393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to stem and lemmatize text, while preserving newlines\n",
    "def preprocess_text(text, use_stemming=True):\n",
    "    # Split the text by newline characters to preserve them\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Process each line separately\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line)  # Tokenize the line\n",
    "        if use_stemming:\n",
    "            tokens = [stemmer.stem(token) for token in tokens]  # Apply stemming\n",
    "        else:\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Apply lemmatization\n",
    "        processed_line = ' '.join(tokens)  # Reassemble the processed tokens into a line\n",
    "        processed_lines.append(processed_line)  # Append the processed line to the list\n",
    "    \n",
    "    # Join the processed lines back with newline characters\n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "# Apply stemming/lemmatization to the specified columns\n",
    "columns_to_process = ['mistral-7b', 'llama3-8b', 'gemma2-9b', 'llama3-70b', 'GPT-4o', 'Annotation']\n",
    "\n",
    "for column in columns_to_process:\n",
    "    # Choose whether to use stemming or lemmatization and preserve newlines\n",
    "    RP_df[column] = RP_df[column].apply(lambda x: preprocess_text(x, use_stemming=True))\n",
    "    RS_df[column] = RS_df[column].apply(lambda x: preprocess_text(x, use_stemming=True))\n",
    "    OP_df[column] = OP_df[column].apply(lambda x: preprocess_text(x, use_stemming=True))\n",
    "    OS_df[column] = OS_df[column].apply(lambda x: preprocess_text(x, use_stemming=True))\n",
    "    no_demo_df[column] = no_demo_df[column].apply(lambda x: preprocess_text(x, use_stemming=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd02a4-a88e-42b4-9e34-c4880728e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd85bc9-5acf-43dd-b970-1982b735db7e",
   "metadata": {},
   "source": [
    "### Annotation_based Accuracy Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893690bd-a81b-4510-9da4-d5282e2bba73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load  # Import the evaluate library for METEOR\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "bleu_scorer = BLEU(effective_order=True)  # Enable effective_order for sentence-level BLEU\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "meteor_scorer = load('meteor')\n",
    "\n",
    "# Function to calculate metrics for each model against the Annotation column\n",
    "def calculate_detailed_metrics(model_column, reference_column):\n",
    "    # Initialize lists to store scores\n",
    "    bleu_detailed_scores = {\n",
    "        \"BLEU\": [],\n",
    "        \"1-gram\": [],\n",
    "        \"2-gram\": [],\n",
    "        \"3-gram\": [],\n",
    "        \"hyp_len\": [],\n",
    "        \"ref_len\": []\n",
    "    }\n",
    "    meteor_scores = []\n",
    "    rouge_detailed_scores = {\n",
    "        \"ROUGE-1\": [], \"ROUGE-2\": [], \"ROUGE-L\": []\n",
    "    }\n",
    "    bert_score_metrics = {\n",
    "        \"BERTScore F1\": []\n",
    "    }\n",
    "\n",
    "    # Iterate over each row to compute scores\n",
    "    for i, row in no_demo_df.iterrows():\n",
    "        candidate = str(row[model_column])  # Ensure candidate and reference are strings\n",
    "        reference = str(row[reference_column])\n",
    "\n",
    "        # BLEU score (SacreBLEU) with detailed components\n",
    "        bleu = bleu_scorer.sentence_score(candidate, [reference])\n",
    "        bleu_detailed_scores[\"BLEU\"].append(bleu.score)\n",
    "        bleu_detailed_scores[\"1-gram\"].append(bleu.precisions[0])\n",
    "        bleu_detailed_scores[\"2-gram\"].append(bleu.precisions[1])\n",
    "        bleu_detailed_scores[\"3-gram\"].append(bleu.precisions[2])\n",
    "        bleu_detailed_scores[\"hyp_len\"].append(bleu.sys_len)\n",
    "        bleu_detailed_scores[\"ref_len\"].append(bleu.ref_len)\n",
    "\n",
    "        # METEOR score using the evaluate library\n",
    "        meteor_result = meteor_scorer.compute(predictions=[candidate], references=[reference])\n",
    "        meteor_scores.append(meteor_result['meteor'])\n",
    "\n",
    "        # ROUGE scores\n",
    "        rouge_score = rouge_scorer.score(reference, candidate)\n",
    "        rouge_detailed_scores[\"ROUGE-1\"].append(rouge_score['rouge1'].fmeasure)\n",
    "        rouge_detailed_scores[\"ROUGE-2\"].append(rouge_score['rouge2'].fmeasure)\n",
    "        rouge_detailed_scores[\"ROUGE-L\"].append(rouge_score['rougeL'].fmeasure)\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=False)\n",
    "        bert_score_metrics[\"BERTScore F1\"].append(F1.mean().item())\n",
    "\n",
    "    # Create DataFrame to store all scores\n",
    "    metrics_df = pd.DataFrame({\n",
    "        **bleu_detailed_scores,\n",
    "        **rouge_detailed_scores,\n",
    "        \"METEOR\": meteor_scores,\n",
    "        **bert_score_metrics\n",
    "    })\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "# Calculate detailed metrics for each model against the Annotation column\n",
    "metrics_mistral_7b_df = calculate_detailed_metrics('mistral-7b', 'Annotation')\n",
    "metrics_llama3_8b_df = calculate_detailed_metrics('llama3-8b', 'Annotation')\n",
    "metrics_gemma2_9b_df = calculate_detailed_metrics('gemma2-9b', 'Annotation')\n",
    "metrics_llama3_70b_df = calculate_detailed_metrics('llama3-70b', 'Annotation')\n",
    "metrics_GPT_4o_df = calculate_detailed_metrics('GPT-4o', 'Annotation')\n",
    "\n",
    "# Calculate average metrics\n",
    "average_mistral_7b_df = metrics_mistral_7b_df.mean()\n",
    "average_llama3_8b_df = metrics_llama3_8b_df.mean()\n",
    "average_gemma2_9b_df = metrics_gemma2_9b_df.mean()\n",
    "average_llama3_70b_df = metrics_llama3_70b_df.mean()\n",
    "average_GPT_4o_df = metrics_GPT_4o_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d366059f-5e18-4b2c-9cc7-a9c380090201",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mistral_7b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998b268-45de-46b6-8c2b-22f7559b1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_llama3_8b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6fbbb-4790-464c-a1f1-c9ffe2277f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_gemma2_9b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e1081-4058-468b-921f-98895df4c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_llama3_70b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2ab58-aade-4910-b71f-52821ac9f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_GPT_4o_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff58c9-422b-4f4a-b3c4-ae566e6b5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load  # Import the evaluate library for METEOR\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "bleu_scorer = BLEU(effective_order=True)  # Enable effective_order for sentence-level BLEU\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "meteor_scorer = load('meteor')\n",
    "\n",
    "# Function to calculate metrics for each model against the Annotation column\n",
    "def calculate_detailed_metrics(model_column, reference_column):\n",
    "    # Initialize lists to store scores\n",
    "    bleu_detailed_scores = {\n",
    "        \"BLEU\": [],\n",
    "        \"1-gram\": [],\n",
    "        \"2-gram\": [],\n",
    "        \"3-gram\": [],\n",
    "        \"hyp_len\": [],\n",
    "        \"ref_len\": []\n",
    "    }\n",
    "    meteor_scores = []\n",
    "    rouge_detailed_scores = {\n",
    "        \"ROUGE-1\": [], \"ROUGE-2\": [], \"ROUGE-L\": []\n",
    "    }\n",
    "    bert_score_metrics = {\n",
    "        \"BERTScore F1\": []\n",
    "    }\n",
    "\n",
    "# Iterate over each row to compute scores\n",
    "    for i, row in OS_df.iterrows():\n",
    "        candidate = str(row[model_column])  # Ensure candidate and reference are strings\n",
    "        reference = str(row[reference_column])\n",
    "\n",
    "        # BLEU score (SacreBLEU) with detailed components\n",
    "        bleu = bleu_scorer.sentence_score(candidate, [reference])\n",
    "        bleu_detailed_scores[\"BLEU\"].append(bleu.score)\n",
    "        bleu_detailed_scores[\"1-gram\"].append(bleu.precisions[0])\n",
    "        bleu_detailed_scores[\"2-gram\"].append(bleu.precisions[1])\n",
    "        bleu_detailed_scores[\"3-gram\"].append(bleu.precisions[2])\n",
    "        bleu_detailed_scores[\"hyp_len\"].append(bleu.sys_len)\n",
    "        bleu_detailed_scores[\"ref_len\"].append(bleu.ref_len)\n",
    "\n",
    "        # METEOR score using the evaluate library\n",
    "        meteor_result = meteor_scorer.compute(predictions=[candidate], references=[reference])\n",
    "        meteor_scores.append(meteor_result['meteor'])\n",
    "\n",
    "        # ROUGE scores\n",
    "        rouge_score = rouge_scorer.score(reference, candidate)\n",
    "        rouge_detailed_scores[\"ROUGE-1\"].append(rouge_score['rouge1'].fmeasure)\n",
    "        rouge_detailed_scores[\"ROUGE-2\"].append(rouge_score['rouge2'].fmeasure)\n",
    "        rouge_detailed_scores[\"ROUGE-L\"].append(rouge_score['rougeL'].fmeasure)\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=False)\n",
    "        bert_score_metrics[\"BERTScore F1\"].append(F1.mean().item())\n",
    "\n",
    "    # Create DataFrame to store all scores\n",
    "    metrics_df = pd.DataFrame({\n",
    "        **bleu_detailed_scores,\n",
    "        **rouge_detailed_scores,\n",
    "        \"METEOR\": meteor_scores,\n",
    "        **bert_score_metrics\n",
    "    })\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "# Calculate detailed metrics for each model against the Annotation column\n",
    "metrics_mistral_7b_df = calculate_detailed_metrics('mistral-7b', 'Annotation')\n",
    "metrics_llama3_8b_df = calculate_detailed_metrics('llama3-8b', 'Annotation')\n",
    "metrics_gemma2_9b_df = calculate_detailed_metrics('gemma2-9b', 'Annotation')\n",
    "metrics_llama3_70b_df = calculate_detailed_metrics('llama3-70b', 'Annotation')\n",
    "metrics_GPT_4o_df = calculate_detailed_metrics('GPT-4o', 'Annotation')\n",
    "\n",
    "# Calculate average metrics\n",
    "average_mistral_7b_df = metrics_mistral_7b_df.mean()\n",
    "average_llama3_8b_df = metrics_llama3_8b_df.mean()\n",
    "average_gemma2_9b_df = metrics_gemma2_9b_df.mean()\n",
    "average_llama3_70b_df = metrics_llama3_70b_df.mean()\n",
    "average_GPT_4o_df = metrics_GPT_4o_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a11614-e812-4e5f-bfd6-f1b542294461",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mistral_7b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6efa5-59e4-4c12-8350-59ea3f396be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_llama3_8b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772671f8-99e2-4bd8-b582-081a8c1f0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_gemma2_9b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd2633-1caf-4dbd-8d76-1ffce79c443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_llama3_70b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afff50-24e9-400b-9814-32ae7385bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_GPT_4o_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2965d-6174-436b-b266-46d4f04808f5",
   "metadata": {},
   "source": [
    "### OC, Hallucinate Eval (hard-coded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37877a4b-2c44-43d9-9c89-952eab324aac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18071b-7f4a-48a0-9836-a7d1f61416f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_df = pd.read_csv('os.path.join(REPORTS_DIR, \"RP_cleaned_output.csv\"))\n",
    "RS_df = pd.read_csv('os.path.join(REPORTS_DIR, \"RS_cleaned_output.csv\"))\n",
    "OP_df = pd.read_csv('os.path.join(REPORTS_DIR, \"OP_cleaned_output.csv\"))\n",
    "OS_df = pd.read_csv('os.path.join(REPORTS_DIR, \"OS_cleaned_output.csv\"))\n",
    "no_demo_df = pd.read_csv('os.path.join(BASE_DIR, \"no_demo_cleaned.csv\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1d0d8-d6fe-49f6-a0f6-b51c155575ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb59da1-408e-406d-83cf-0dc62372ec6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_relation(triple):\n",
    "    # Extract the part before '(' as the relation\n",
    "    match = re.match(r'([^\\(]+)\\(', triple)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return the relation, strip extra spaces\n",
    "    return None\n",
    "\n",
    "# Calculate Ontology Conformance and identify non-conformant relations\n",
    "def calculate_ontology_conformance(output_triples, allowed_relations):\n",
    "    conformant_triples = 0\n",
    "    total_triples = len(output_triples)\n",
    "    non_conformant_relations = []\n",
    "\n",
    "    for triple in output_triples:\n",
    "        relation = extract_relation(triple)\n",
    "        if relation and relation in allowed_relations:\n",
    "            conformant_triples += 1\n",
    "        else:\n",
    "            if relation: \n",
    "                non_conformant_relations.append(relation)\n",
    "\n",
    "    # Compute the ontology conformance metric\n",
    "    oc_metric = (conformant_triples / total_triples) * 100 if total_triples > 0 else 0\n",
    "    return oc_metric, non_conformant_relations\n",
    "\n",
    "# Function to process the entire DataFrame\n",
    "def process_dataframe(df):\n",
    "    # Initialize lists to store results\n",
    "    oc_metrics = {model: [] for model in ['mistral-7b', 'llama3-8b', 'gemma2-9b', 'llama3-70b', 'GPT-4o']}\n",
    "    wrong_relations = {model: [] for model in ['mistral-7b', 'llama3-8b', 'gemma2-9b', 'llama3-70b', 'GPT-4o']}\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        # Extract allowed relations from the prompt (ontology)\n",
    "        prompt_relations = extract_relations_from_prompt(row['prompts_with_no_demo'])  # Extract from the prompt column\n",
    "\n",
    "        # Process each model's output\n",
    "        for model in oc_metrics.keys():\n",
    "            output_triples = row[model].split('\\n')  # Split model's output by newline\n",
    "            output_triples = [t.strip() for t in output_triples if t.strip()]  # Clean up the triples\n",
    "\n",
    "            # Calculate ontology conformance\n",
    "            oc_metric, non_conformant_relations = calculate_ontology_conformance(output_triples, prompt_relations)\n",
    "\n",
    "            # Store the results\n",
    "            oc_metrics[model].append(oc_metric)\n",
    "            wrong_relations[model].append(non_conformant_relations)\n",
    "\n",
    "    # Add the results to the DataFrame\n",
    "    for model in oc_metrics.keys():\n",
    "        df[f'{model}_OC'] = oc_metrics[model]\n",
    "        df[f'{model}_NonConformantRelations'] = wrong_relations[model]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to extract allowed relations from the prompt\n",
    "def extract_relations_from_prompt(prompt):\n",
    "    match = re.search(r'relation_types:\\s*(.*)', prompt)\n",
    "    if match:\n",
    "        relations_str = match.group(1)\n",
    "        allowed_relations = set(map(str.strip, relations_str.split(',')))  # Convert to a set of allowed relations\n",
    "        return allowed_relations\n",
    "    else:\n",
    "        raise ValueError(\"Relation types not found in the prompt.\")\n",
    "\n",
    "# Example DataFrame processing\n",
    "no_demo_df_processed = process_dataframe(no_demo_df)\n",
    "\n",
    "# Calculate the average OC score for each model\n",
    "average_oc_scores = {\n",
    "    'mistral-7b': no_demo_df_processed['mistral-7b_OC'].mean(),\n",
    "    'llama3-8b': no_demo_df_processed['llama3-8b_OC'].mean(),\n",
    "    'gemma2-9b': no_demo_df_processed['gemma2-9b_OC'].mean(),\n",
    "    'llama3-70b': no_demo_df_processed['llama3-70b_OC'].mean(),\n",
    "    'GPT-4o': no_demo_df_processed['GPT-4o_OC'].mean()\n",
    "}\n",
    "\n",
    "# Display the average OC scores\n",
    "for model, avg_oc in average_oc_scores.items():\n",
    "    print(f\"Average OC for {model}: {avg_oc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a53d5e-31e7-4c4d-8d92-8c589eae12d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RS_df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad206709-9a27-4b44-8977-14835ee36dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count frequency of non-conformant relation output by each model\n",
    "\n",
    "# Define the columns containing non-conformant relations\n",
    "nonconformant_columns = [\n",
    "    'mistral-7b_NonConformantRelations', \n",
    "    'llama3-8b_NonConformantRelations', \n",
    "    'gemma2-9b_NonConformantRelations', \n",
    "    'llama3-70b_NonConformantRelations', \n",
    "    'GPT-4o_NonConformantRelations'\n",
    "]\n",
    "\n",
    "# Function to count non-conformant relation frequencies\n",
    "def count_nonconformant_frequencies(column_data):\n",
    "    all_relations = []\n",
    "    for relations in column_data:\n",
    "        # Extend the list with non-conformant relations directly\n",
    "        all_relations.extend(relations)\n",
    "    \n",
    "    # Count the frequency of each relation using Counter\n",
    "    return Counter(all_relations)\n",
    "\n",
    "# Create a dictionary to store the frequency counts for each column\n",
    "frequency_results = {}\n",
    "\n",
    "# Process each non-conformant relations column\n",
    "for column in nonconformant_columns:\n",
    "    frequency_results[column] = count_nonconformant_frequencies(no_demo_df_processed[column])\n",
    "\n",
    "# Print out the top 10 most frequent non-conformant relations for each model\n",
    "for model, counter in frequency_results.items():\n",
    "    print(f\"\\nTop 10 most frequent non-conformant relations for {model} (sorted by frequency):\")\n",
    "    # Sort the counter by frequency (most frequent first) and take the top 10\n",
    "    sorted_relations = sorted(counter.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "    for relation, freq in sorted_relations:\n",
    "        print(f\"{relation}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2ae3c-bafe-4170-bf74-63cce8bad5b8",
   "metadata": {},
   "source": [
    "#### Format Conformance and Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f4cf01-4a35-4aa1-adb3-51514bf2f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_df = pd.read_csv('os.path.join(REPORTS_DIR, \"RP_cleaned_output.csv\")) # two Sentence \n",
    "RS_df = pd.read_csv('os.path.join(REPORTS_DIR, \"RS_cleaned_output.csv\")) # two Sentence \n",
    "OP_df = pd.read_csv('os.path.join(REPORTS_DIR, \"OP_cleaned_output.csv\")) # two Paragraph\n",
    "OS_df = pd.read_csv('os.path.join(REPORTS_DIR, \"OS_cleaned_output.csv\")) # two Sentence \n",
    "no_demo_df = pd.read_csv('os.path.join(BASE_DIR, \"no_demo_cleaned.csv\")') # one Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b665065-993e-4592-96ca-77ca55783880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271acf72-ffd7-4a41-bd0c-062e76bb9218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to validate the triple format\n",
    "def is_valid_triple(triple):\n",
    "    # Updated regex to handle phrases with special characters\n",
    "    match = re.match(r'^[a-zA-Z_]+\\([^()]+(?:, [^()]+)+\\)$', triple)\n",
    "    return bool(match)\n",
    "\n",
    "# Function to normalize text (lowercase and strip extra spaces)\n",
    "def normalize_text(text):\n",
    "    # Replace newline characters with spaces and normalize spaces\n",
    "    return re.sub(r'\\s+', ' ', text.replace('\\n', ' ').strip().lower())\n",
    "\n",
    "# Function to extract the second sentence from the prompt\n",
    "def extract_context_from_prompt(prompt): # RP_df  RS_df\n",
    "    # Match all occurrences of \"Sentence:\" and extract the following content\n",
    "    matches = re.findall(r'Sentence:\\s*(.*?)\\s*(?=Sentence:|\\Z)', prompt, re.DOTALL)\n",
    "    if len(matches) > 1:\n",
    "        context = matches[1]  # Extract the second match\n",
    "        normalized_context = normalize_text(context)\n",
    "        # print(f\"Extracted Context: {normalized_context}\")  # Debugging: Print the extracted context\n",
    "        return normalized_context\n",
    "    elif len(matches) == 1:\n",
    "        # print(f\"Only one sentence found in prompt, using the available context:\\n{matches[0]}\")\n",
    "        return normalize_text(matches[0])\n",
    "    else:\n",
    "        # print(f\"No valid context sentence found in the prompt: {prompt}\")\n",
    "        raise ValueError(\"Second context sentence not found in the prompt.\")\n",
    "\n",
    "# Function to check if the subject and object in the triple are in the context\n",
    "def detect_hallucinations(triples, context):\n",
    "    subject_hallucinations = 0\n",
    "    object_hallucinations = 0\n",
    "    valid_triples = 0\n",
    "    invalid_triples_list = []  # List to store invalid triples\n",
    "    \n",
    "    for index, triple in enumerate(triples):\n",
    "        # if index >= 5:  # Process only the first five triples for debugging\n",
    "        #     break\n",
    "\n",
    "        if not is_valid_triple(triple):\n",
    "            print(f\"Ill-formatted triple detected: {triple}\")\n",
    "            invalid_triples_list.append(triple)  # Store the invalid triple\n",
    "            continue  # Skip invalid triples\n",
    "\n",
    "        valid_triples += 1\n",
    "\n",
    "        try:\n",
    "            # Parse the triple\n",
    "            relation, entities = triple.split('(', 1)\n",
    "            subject, obj = entities[:-1].split(', ')\n",
    "            \n",
    "            # Normalize subject and object\n",
    "            subject_normalized = normalize_text(subject)\n",
    "            object_normalized = normalize_text(obj)\n",
    "            \n",
    "            # Print the subject and object being compared\n",
    "            # print(f\"Processing Triple: {triple}\")\n",
    "            # print(f\"Subject: {subject_normalized} | Object: {object_normalized}\")\n",
    "            \n",
    "            # Check for subject hallucinations\n",
    "            if subject_normalized not in context:\n",
    "                print(f\"Subject Hallucination Detected: {subject_normalized} not found in context.\")\n",
    "                subject_hallucinations += 1\n",
    "                \n",
    "            # Check for object hallucinations\n",
    "            if object_normalized not in context:\n",
    "                print(f\"Object Hallucination Detected: {object_normalized} not found in context.\")\n",
    "                object_hallucinations += 1\n",
    "        \n",
    "        except ValueError:\n",
    "            # Handle cases where the triple is malformed\n",
    "            print(f\"Malformed triple detected: {triple}\")\n",
    "            invalid_triples_list.append(triple)  # Store the malformed triple\n",
    "            continue\n",
    "    \n",
    "    return subject_hallucinations, object_hallucinations, valid_triples, invalid_triples_list\n",
    "\n",
    "# Function to process the entire DataFrame and calculate hallucinations\n",
    "def process_dataframe_for_hallucinations(df):\n",
    "    hallucination_counts = {\n",
    "        model: {'subject': 0, 'object': 0, 'total_triples': 0, 'invalid_triples': 0, 'all_triples': 0, 'invalid_triples_list': []}\n",
    "        for model in ['mistral-7b', 'llama3-8b', 'gemma2-9b', 'llama3-70b', 'GPT-4o']\n",
    "    }\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        prompt = row['prompts_with_no_demo']\n",
    "        try:\n",
    "            context = extract_context_from_prompt(prompt)\n",
    "        except ValueError:\n",
    "            print(f\"Skipping row {i} due to missing context.\")\n",
    "            continue\n",
    "        \n",
    "        for model in hallucination_counts.keys():\n",
    "            triples = row[model].split('\\n')\n",
    "            \n",
    "            hallucination_counts[model]['all_triples'] += len(triples)  # Count all triples\n",
    "\n",
    "            # Process all triples to detect hallucinations\n",
    "            subject_hallucinations, object_hallucinations, valid_triple_count, invalid_triples = detect_hallucinations(triples, context)\n",
    "            \n",
    "            # Update counts\n",
    "            hallucination_counts[model]['subject'] += subject_hallucinations\n",
    "            hallucination_counts[model]['object'] += object_hallucinations\n",
    "            hallucination_counts[model]['total_triples'] += valid_triple_count\n",
    "            hallucination_counts[model]['invalid_triples'] += len(invalid_triples)\n",
    "            hallucination_counts[model]['invalid_triples_list'].extend(invalid_triples)\n",
    "    \n",
    "    # Calculate the percentage of hallucinations and invalid triples\n",
    "    hallucination_percentages = {}\n",
    "    for model in hallucination_counts.keys():\n",
    "        total_valid_triples = hallucination_counts[model]['total_triples']\n",
    "        total_triples = hallucination_counts[model]['all_triples']\n",
    "        invalid_triples = hallucination_counts[model]['invalid_triples']\n",
    "\n",
    "        if total_valid_triples > 0:\n",
    "            subject_percentage = (hallucination_counts[model]['subject'] / total_valid_triples) * 100\n",
    "            object_percentage = (hallucination_counts[model]['object'] / total_valid_triples) * 100\n",
    "        else:\n",
    "            subject_percentage = 0.0\n",
    "            object_percentage = 0.0\n",
    "        \n",
    "        invalid_triple_percentage = (invalid_triples / total_triples) * 100 if total_triples > 0 else 0.0\n",
    "        \n",
    "        hallucination_percentages[model] = {\n",
    "            'subject_percentage': subject_percentage,\n",
    "            'object_percentage': object_percentage,\n",
    "            'invalid_triple_percentage': invalid_triple_percentage,\n",
    "            'invalid_triples': invalid_triples,\n",
    "            'invalid_triples_list': hallucination_counts[model]['invalid_triples_list']  # Store the list of invalid triples\n",
    "        }\n",
    "    \n",
    "    return hallucination_percentages\n",
    "\n",
    "# Usage\n",
    "# df = pd.read_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics.csv\"))\n",
    "hallucination_results = process_dataframe_for_hallucinations(no_demo_df)\n",
    "\n",
    "# Print out the results\n",
    "for model, percentages in hallucination_results.items():\n",
    "    print(f\"\\nHallucination percentages for {model}:\")\n",
    "    print(f\"Subject Hallucinations: {percentages['subject_percentage']:.2f}%\")\n",
    "    print(f\"Object Hallucinations: {percentages['object_percentage']:.2f}%\")\n",
    "    print(f\"Proportion of Invalid Triples: {percentages['invalid_triple_percentage']:.2f}%\")\n",
    "    print(f\"Invalid Triples: {percentages['invalid_triples']} found\")\n",
    "    print(\"Invalid Triples List:\")\n",
    "    for invalid_triple in percentages['invalid_triples_list']:\n",
    "        print(f\" - {invalid_triple}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6344f3-2851-4720-9164-2d6320b923d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model, percentages in hallucination_results.items():\n",
    "    format_conformance = 100 - percentages['invalid_triple_percentage']  # Calculate Format Conformance\n",
    "    print(f\"\\nHallucination percentages for {model}:\")\n",
    "    print(f\"Subject Hallucinations: {percentages['subject_percentage']:.2f}%\")\n",
    "    print(f\"Object Hallucinations: {percentages['object_percentage']:.2f}%\")\n",
    "    print(f\"Format Conformance: {format_conformance:.2f}%\")  # Print Format Conformance\n",
    "    print(f\"Invalid Triples: {percentages['invalid_triples']} found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717aec3-7cec-4124-b9b4-54ee447aa195",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LLM judges (GPT-4o, llama3-70b, llama3.1-70b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4547d-3bdb-4de9-90df-565992b9194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from openai import AzureOpenAI\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15d471-9cd8-4e10-9ff2-37418681481b",
   "metadata": {},
   "source": [
    "#### Accuracy Judge Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f3f49-f07b-4334-8d5a-0d23bdcb8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31605a21-5990-4a2d-abc7-611e0861e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "accuracy_judge_prompt_template = '''\n",
    "You are a judge who ranks five models from 1 to 5 on a triple extraction task. You must assign 1 to the model with the best answer and 5 to the model with the worst answer. Your ranking should be provided directly in this format: [1: model x; 2: model x; 3: model x; 4: model x; 5: model x]. \n",
    "Ranking Criteria: \n",
    "Correctness of Triples: The triples must conform to the format relation(subject, object) and must accurately reflect relationships stated in the context. Models with significant formatting errors should be penalized. \n",
    "Coverage: The number of correct triples extracted. More accurate triples are better, but avoid penalizing slight redundancies unless they detract from the overall relevance. \n",
    "Relevance: The triples must be relevant to the specified entity and relation types and should align well with the specific context provided. \n",
    "Edge Cases: If a model extracts many triples but includes incorrect or redundant ones, balance accuracy and redundancy in your ranking. Correctness should be prioritized, followed by relevance, then coverage. \n",
    "Given Entity Types: {entity_types}\n",
    "Given Relation Types: {relation_types} \n",
    "Context: {context}\n",
    "Model Outputs: \n",
    "Model 1: {model_1}\n",
    "Model 2: {model_2}\n",
    "Model 3: {model_3}\n",
    "Model 4: {model_4}\n",
    "Model 5: {model_5}\n",
    "Your ranking:\n",
    "'''\n",
    "\n",
    "# Function to extract specific parts from the 'Prompts' column\n",
    "def extract_parts_from_prompt(prompt):\n",
    "    entity_types = re.search(r'entity_types:\\s*(.*?)\\s*relation_types:', prompt, re.DOTALL)\n",
    "    relation_types = re.search(r'relation_types:\\s*(.*?)\\s*Example:', prompt, re.DOTALL)\n",
    "    sentences = re.findall(r'Sentence:\\s*(.*?)\\s*(?=Sentence:|\\Z)', prompt, re.DOTALL)\n",
    "    \n",
    "    # Use the second sentence as the context\n",
    "    context = sentences[1].strip() if len(sentences) > 1 else sentences[0].strip()    \n",
    "    \n",
    "    return (entity_types.group(1).strip() if entity_types else \"\",\n",
    "            relation_types.group(1).strip() if relation_types else \"\",\n",
    "            context)\n",
    "\n",
    "# Create the 'Accuracy_judge_prompts' column by applying the template to each row\n",
    "df['Accuracy_judge_prompts'] = df.apply(\n",
    "    lambda row: accuracy_judge_prompt_template.format(\n",
    "        entity_types=extract_parts_from_prompt(row['Prompts'])[0],\n",
    "        relation_types=extract_parts_from_prompt(row['Prompts'])[1],\n",
    "        context=extract_parts_from_prompt(row['Prompts'])[2],\n",
    "        model_1=row['llama3'],\n",
    "        model_2=row['vicuna'],\n",
    "        model_3=row['mistral-instruct'],\n",
    "        model_4=row['GPT-4o'],\n",
    "        model_5=row['llama3-70b']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782ce42-1538-45fe-a512-80ec4ac9eb89",
   "metadata": {},
   "source": [
    "#### GPT judges Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44b418-bb11-436b-8963-a1171e0b9d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OpenAI \n",
    "prompts = pd.read_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"))\n",
    "output_csv_file_path = 'os.path.join(EVAL_DIR, \"GPT_judge_Accuracy.csv\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint=\"https://cmac-openai-default.openai.azure.com/\", \n",
    "  api_key=\"<KEY TO BE ADDED>\",  \n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 5\n",
    "retry_delay = 8  # seconds\n",
    "\n",
    "try:\n",
    "    # Iterate through each prompt\n",
    "    # for i, row in prompts.iloc[170:].iterrows():\n",
    "    for i, row in prompts.iterrows():\n",
    "        prompt = row['Accuracy_judge_prompts']\n",
    "        print(f\"Processing prompt {i + 1} out of {len(prompts)}\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-default\", \n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "                # end_time = time.time()\n",
    "                # processing_time = end_time - start_time\n",
    "                \n",
    "                # Update the DataFrame with the response and processing time\n",
    "                prompts.at[i, 'GPT-Accuracy'] = response.choices[0].message.content\n",
    "                \n",
    "                print(f\"Model's Answer for prompt {i + 1}:\\n{response.choices[0].message.content}\\n\")\n",
    "                \n",
    "                # Save the updated DataFrame back to a CSV file after each prompt\n",
    "                prompts.to_csv(output_csv_file_path, index=False)\n",
    "                \n",
    "                # Break the retry loop if the request was successful\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                if '429' in str(e):\n",
    "                    retry_count += 1\n",
    "                    print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063bb1d-e8f7-4016-9780-59ba2b814289",
   "metadata": {},
   "source": [
    "#### GPT judges OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ad596-db65-4180-83ad-c3ed5ce4f16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OpenAI \n",
    "prompts = pd.read_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"))\n",
    "output_csv_file_path = 'os.path.join(EVAL_DIR, \"GPT_judge_OC74-.csv\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint=\"https://cmac-openai-default.openai.azure.com/\", \n",
    "  api_key=\"8f7786fd7cca4bbeb86fcde25a394fa8\",  \n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 5\n",
    "retry_delay = 8  # seconds\n",
    "\n",
    "try:\n",
    "    # Iterate through each prompt\n",
    "    for i, row in prompts.iloc[74:].iterrows():\n",
    "    # for i, row in prompts.iterrows():\n",
    "        prompt = row['OC_judge_prompts']\n",
    "        print(f\"Processing prompt {i + 1} out of {len(prompts)}\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-default\", \n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "                # end_time = time.time()\n",
    "                # processing_time = end_time - start_time\n",
    "                \n",
    "                # Update the DataFrame with the response and processing time\n",
    "                prompts.at[i, 'GPT-OC'] = response.choices[0].message.content\n",
    "                \n",
    "                print(f\"Model's Answer for prompt {i + 1}:\\n{response.choices[0].message.content}\\n\")\n",
    "                \n",
    "                # Save the updated DataFrame back to a CSV file after each prompt\n",
    "                prompts.to_csv(output_csv_file_path, index=False)\n",
    "                \n",
    "                # Break the retry loop if the request was successful\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                if '429' in str(e):\n",
    "                    retry_count += 1\n",
    "                    print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce2956-5f77-47f1-8d18-2acb2783646b",
   "metadata": {},
   "source": [
    "#### GPT judges FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113f6b3-5ce0-4006-916d-4d7bc1db7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Accuracy_GPT_judge_llama3\"\n",
    "\"Accuracy_GPT_judge_vicuna\"\n",
    "\"Accuracy_GPT_judge_mistral\"\n",
    "\"Accuracy_GPT_judge_GPT\"\n",
    "\"Accuracy_GPT_judge_llama3-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75649aa8-7df8-4d8b-a988-d53b8f5ae8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Accuracy_llama3-70b_judge_llama3\"\n",
    "\"Accuracy_llama3-70b_judge_vicuna\"\n",
    "\"Accuracy_llama3-70b_judge_mistral\"\n",
    "\"Accuracy_llama3-70b_judge_GPT\"\n",
    "\"Accuracy_llama3-70b_judge_llama3-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536ea29-a35a-4490-ae44-433f506b0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Accuracy_llama3.1-70b_judge_llama3\"\n",
    "\"Accuracy_llama3.1-70b_judge_vicuna\"\n",
    "\"Accuracy_llama3.1-70b_judge_mistral\"\n",
    "\"Accuracy_llama3.1-70b_judge_GPT\"\n",
    "\"Accuracy_llama3.1-70b_judge_llama3-70b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990e038-4a74-4f4a-aa75-562c97c55b24",
   "metadata": {},
   "source": [
    "#### OC Judge Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9dcfe-3964-435d-b13c-418cc1bb055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8015a-3473-48f5-a9d8-3fbd7bd8eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "OC_judge_prompt_template = '''\n",
    "You are a judge who checks whether the relation type in each triple is conformant to the given relation types. The triples are in the format relation(subject, object). You must assign 1 if the relation in a triple is conformant and assign 0 if it is not. Your check results should be provided directly in this format: [x1, x2, ..., xn] where x is a boolean value (1 or 0), and n is the number of triples. Do not include any additional explanations or text, just output the results in the format specified above.\n",
    "Checking Criteria:\n",
    "A relation is conformant if it is present in the given relation types; otherwise, it is not conformant.\n",
    "Given Relation Types: {relation_types}\n",
    "Model Outputs:\n",
    "Model 1: {model_1}\n",
    "Model 2: {model_2}\n",
    "Model 3: {model_3}\n",
    "Model 4: {model_4}\n",
    "Model 5: {model_5}\n",
    "Your output:\n",
    "'''\n",
    "\n",
    "# Function to extract specific parts from the 'Prompts' column\n",
    "def extract_parts_from_prompt(prompt):\n",
    "    relation_types = re.search(r'relation_types:\\s*(.*?)\\s*Example:', prompt, re.DOTALL)    \n",
    "    return relation_types.group(1).strip() if relation_types else \"\"\n",
    "\n",
    "# Create the 'OC_judge_prompts' column by applying the template to each row\n",
    "df['OC_judge_prompts'] = df.apply(\n",
    "    lambda row: OC_judge_prompt_template.format(\n",
    "        relation_types=extract_parts_from_prompt(row['Prompts']),\n",
    "        model_1=row['llama3'],\n",
    "        model_2=row['vicuna'],\n",
    "        model_3=row['mistral-instruct'],\n",
    "        model_4=row['GPT-4o'],\n",
    "        model_5=row['llama3-70b']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda7008-71a7-41c0-9e61-6bfb6431607b",
   "metadata": {},
   "source": [
    "#### FC Judge Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5643ad-5ee3-4906-b3d6-bee081bf5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58027b15-71fb-4411-9899-fe976ad32140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "FC_judge_prompt_template = '''\n",
    "You are a judge who checks whether each triple is conformant to the format. A triple is considered conformant to the format if it strictly follows the pattern: relation(subject, object). In this pattern, relation is a string of alphabetic characters (e.g., hasLocation). Subject and object are strings enclosed in parentheses, separated by a comma, and may contain special characters, numbers, and spaces.\n",
    "Checking Criteria:\n",
    "If the triple is conformant to the format, return 1; otherwise, return 0. In the output, you must directly provide a list [x1, x2, ..., xn] where x is a boolean value (1 or 0), and n is the number of triples. Do not include any additional explanations or text; just output the results in the format specified above.\n",
    "Edge Cases That Should Be Considered Conformant:\n",
    "If the subject or object in the triple contains numbers with commas (e.g., 2,500,011 square meters), it should be considered conformant. This allows for large numbers to be formatted correctly.\n",
    "If the subject or object contains an acronym or abbreviation within parentheses, it should be considered conformant. This allows for organization names, project names, etc., that include acronyms.\n",
    "Model Outputs: \n",
    "Model 1: {model_1}\n",
    "Model 2: {model_2}\n",
    "Model 3: {model_3}\n",
    "Model 4: {model_4}\n",
    "Model 5: {model_5}\n",
    "Your output:\n",
    "'''\n",
    "\n",
    "# Create the 'FC_judge_prompts' column by applying the template to each row\n",
    "df['FC_judge_prompts'] = df.apply(\n",
    "    lambda row: FC_judge_prompt_template.format(\n",
    "        model_1=row['llama3'],\n",
    "        model_2=row['vicuna'],\n",
    "        model_3=row['mistral-instruct'],\n",
    "        model_4=row['GPT-4o'],\n",
    "        model_5=row['llama3-70b']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('os.path.join(EVAL_DIR, \"A1_OC_metrics_with_judge_prompts.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}