{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION - Edit this section for each construction task\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Get the base directory dynamically\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__ if '__file__' in globals() else '.')))\n",
    "\n",
    "construction_config = {\n",
    "    # Choose construction strategy\n",
    "    \"strategy\": \"DOMAIN_PARAGRAPH\",  # Options: DOMAIN_PARAGRAPH, RANDOM_PARAGRAPH, DOMAIN_SENTENCE\n",
    "    \n",
    "    # Example configuration\n",
    "    \"random_example\": {\n",
    "        \"sentence\": \"**Germany Vallon VMH3**: In June 2023, CMAC's R&D teams additionally conducted another important study on the use and maintenance of the Germany Vallon VMH3 mine detector machine. The study focused on testing the detector head (60 mm) to detect and signalize metal under the ground.\",\n",
    "        \"output\": \"isSupportedByOrganization(study, CMAC's R&D teams)\\nisPartOf(Germany Vallon VMH3, mine detector machine)\\nCausedBy(infrastructure damage, old wartime munitions)\"\n",
    "    },\n",
    "    \n",
    "    # Processing settings\n",
    "    \"debug_output\": True,\n",
    "    \"save_intermediate\": True\n",
    "}\n",
    "\n",
    "# Predefined strategy configurations\n",
    "strategy_configs = {\n",
    "    \"DOMAIN_PARAGRAPH\": {\n",
    "        \"input_annotations\": os.path.join(BASE_DIR, \"eval\", \"Annotations.csv\"),\n",
    "        \"output_csv\": os.path.join(BASE_DIR, \"prompts\", \"domain_paragraph_prompts.csv\"),\n",
    "        \"description\": \"Create domain-specific paragraph examples using ontology-based matching\"\n",
    "    },\n",
    "    \"RANDOM_PARAGRAPH\": {\n",
    "        \"input_prompts\": os.path.join(BASE_DIR, \"prompts\", \"prompts_with_onto_demo.csv\"),\n",
    "        \"output_csv\": os.path.join(BASE_DIR, \"prompts\", \"random_paragraph_prompts.csv\"),\n",
    "        \"description\": \"Replace examples with random paragraph text\"\n",
    "    },\n",
    "    \"DOMAIN_SENTENCE\": {\n",
    "        \"input_annotations\": os.path.join(BASE_DIR, \"eval\", \"Annotations.csv\"),\n",
    "        \"output_csv\": os.path.join(BASE_DIR, \"prompts\", \"domain_sentence_prompts.csv\"),\n",
    "        \"description\": \"Create domain-specific sentence-level examples\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply predefined config if specified\n",
    "if construction_config[\"strategy\"] in strategy_configs:\n",
    "    config = strategy_configs[construction_config[\"strategy\"]]\n",
    "    construction_config.update(config)\n",
    "\n",
    "print(f\"Strategy: {construction_config['strategy']}\")\n",
    "print(f\"Description: {construction_config.get('description', 'Custom strategy')}\")\n",
    "print(f\"Input Annotations: {construction_config.get('input_annotations', 'Not specified')}\")\n",
    "print(f\"Input Prompts: {construction_config.get('input_prompts', 'Not specified')}\")\n",
    "print(f\"Output CSV: {construction_config.get('output_csv', 'Not specified')}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def extract_entity_relation_types(prompt):\n",
    "    \"\"\"Extract entity_types and relation_types from a prompt\"\"\"\n",
    "    entity_types_match = re.search(r'entity_types:(.*?)\\n', prompt, re.DOTALL)\n",
    "    relation_types_match = re.search(r'relation_types:(.*?)\\n', prompt, re.DOTALL)\n",
    "    \n",
    "    entity_types = entity_types_match.group(1).strip() if entity_types_match else None\n",
    "    relation_types = relation_types_match.group(1).strip() if relation_types_match else None\n",
    "    \n",
    "    return entity_types, relation_types\n",
    "\n",
    "def extract_sentence_from_prompt(prompt, sentence_number=2):\n",
    "    \"\"\"Extract the Nth sentence from a prompt (default: second sentence)\"\"\"\n",
    "    sentences = re.split(r'Sentence:', prompt)\n",
    "    \n",
    "    if len(sentences) > sentence_number:\n",
    "        sentence_block = sentences[sentence_number].strip()\n",
    "        sentence = re.split(r'(Output:|Sentence:)', sentence_block)[0].strip()\n",
    "        return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_annotation_for_prompt(df, prompt):\n",
    "    \"\"\"Get the corresponding annotation for a given prompt\"\"\"\n",
    "    row = df[df['Prompts'] == prompt]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['Annotation']\n",
    "    return None\n",
    "\n",
    "def replace_example_in_prompt(prompt, new_example_text):\n",
    "    \"\"\"Replace the example section in a prompt with new text\"\"\"\n",
    "    new_prompt = re.sub(r'Example:.*?Output:.*?(?=\\n|$)', new_example_text, prompt, flags=re.DOTALL)\n",
    "    return new_prompt\n",
    "\n",
    "def create_example_text(sentence, output):\n",
    "    \"\"\"Create formatted example text\"\"\"\n",
    "    return f'''Example: \n",
    "Sentence: {sentence}\n",
    "Output: {output}\n",
    "'''\n",
    "\n",
    "print(\"Utility functions loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN PARAGRAPH STRATEGY\n",
    "def construct_domain_paragraph_prompts(config):\n",
    "    \"\"\"\n",
    "    Create domain-specific paragraph examples using ontology-based matching.\n",
    "    This replicates the functionality of Construct_Prompts_with_domain_Paragraph.ipynb\n",
    "    \"\"\"\n",
    "    print(\"Starting domain paragraph construction...\")\n",
    "    \n",
    "    # Load annotations\n",
    "    df = pd.read_csv(config['input_annotations'])\n",
    "    print(f\"Loaded {len(df)} annotations from {config['input_annotations']}\")\n",
    "    \n",
    "    # Extract entity and relation types\n",
    "    df['entity_types'], df['relation_types'] = zip(*df['Prompts'].apply(extract_entity_relation_types))\n",
    "    \n",
    "    # Find shortest and second shortest matched prompts\n",
    "    shortest_matched_prompts = []\n",
    "    second_shortest_matched_prompts = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        current_entity_types = row['entity_types']\n",
    "        current_relation_types = row['relation_types']\n",
    "        \n",
    "        # Find matching prompts with same entity/relation types\n",
    "        matching_prompts = df[(df['entity_types'] == current_entity_types) & \n",
    "                             (df['relation_types'] == current_relation_types)]\n",
    "        \n",
    "        if config.get('debug_output', False):\n",
    "            print(f\"\\nRow {index}: Found {len(matching_prompts)} matching prompts\")\n",
    "            print(f\"Entity types: {current_entity_types}\")\n",
    "            print(f\"Relation types: {current_relation_types}\")\n",
    "        \n",
    "        if matching_prompts.empty:\n",
    "            shortest_matched_prompts.append(None)\n",
    "            second_shortest_matched_prompts.append(None)\n",
    "            continue\n",
    "        \n",
    "        # Sort by annotation length\n",
    "        matching_prompts = matching_prompts.sort_values(by='Annotation', key=lambda x: x.str.len())\n",
    "        \n",
    "        # Get shortest annotation prompt\n",
    "        shortest_annotation_prompt = matching_prompts.iloc[0]['Prompts']\n",
    "        shortest_matched_prompts.append(shortest_annotation_prompt)\n",
    "        \n",
    "        # Get second shortest if available\n",
    "        if len(matching_prompts) > 1:\n",
    "            second_shortest_annotation_prompt = matching_prompts.iloc[1]['Prompts']\n",
    "            second_shortest_matched_prompts.append(second_shortest_annotation_prompt)\n",
    "        else:\n",
    "            second_shortest_matched_prompts.append(None)\n",
    "    \n",
    "    # Add matched prompts to dataframe\n",
    "    df['Shortest_Matched_Prompt'] = shortest_matched_prompts\n",
    "    df['Second_shortest_Matched_Prompt'] = second_shortest_matched_prompts\n",
    "    \n",
    "    # Extract paragraphs from prompts\n",
    "    df['Paragraph'] = df['Prompts'].apply(lambda prompt: extract_sentence_from_prompt(prompt) if pd.notna(prompt) else None)\n",
    "    df['Paragraph_shortest'] = df['Shortest_Matched_Prompt'].apply(lambda prompt: extract_sentence_from_prompt(prompt) if pd.notna(prompt) else None)\n",
    "    df['Paragraph_second_shortest'] = df['Second_shortest_Matched_Prompt'].apply(lambda prompt: extract_sentence_from_prompt(prompt) if pd.notna(prompt) else None)\n",
    "    \n",
    "    # Get corresponding annotations\n",
    "    df['Annotation_shortest'] = df['Shortest_Matched_Prompt'].apply(lambda prompt: get_annotation_for_prompt(df, prompt) if pd.notna(prompt) else None)\n",
    "    df['Annotation_second_shortest'] = df['Second_shortest_Matched_Prompt'].apply(lambda prompt: get_annotation_for_prompt(df, prompt) if pd.notna(prompt) else None)\n",
    "    \n",
    "    # Create ontology-based prompts\n",
    "    df['prompts_with_ontology_demo'] = df.apply(lambda row: \n",
    "        replace_example_in_prompt(row['Prompts'], \n",
    "                                create_example_text(row['Paragraph_shortest'], row['Annotation_shortest'])) \n",
    "        if pd.notna(row['Paragraph_shortest']) and pd.notna(row['Annotation_shortest']) \n",
    "        else row['Prompts'], axis=1)\n",
    "    \n",
    "    print(f\"Domain paragraph construction completed. Generated {len(df)} prompts.\")\n",
    "    return df\n",
    "\n",
    "print(\"Domain paragraph strategy function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM PARAGRAPH STRATEGY\n",
    "def construct_random_paragraph_prompts(config):\n",
    "    \"\"\"\n",
    "    Replace examples with random paragraph text.\n",
    "    This replicates the functionality of Construct_Prompts_with_random_Paragraph.ipynb\n",
    "    \"\"\"\n",
    "    print(\"Starting random paragraph construction...\")\n",
    "    \n",
    "    # Load existing prompts\n",
    "    df = pd.read_csv(config['input_prompts'])\n",
    "    print(f\"Loaded {len(df)} prompts from {config['input_prompts']}\")\n",
    "    \n",
    "    # Create new example text using configured random example\n",
    "    new_example_text = create_example_text(\n",
    "        config['random_example']['sentence'],\n",
    "        config['random_example']['output']\n",
    "    )\n",
    "    \n",
    "    # Apply the replacement to each prompt\n",
    "    df['prompt_with_random_paragraph'] = df['Prompts'].apply(\n",
    "        lambda prompt: replace_example_in_prompt(prompt, new_example_text)\n",
    "    )\n",
    "    \n",
    "    # Create final output with renamed columns\n",
    "    output_df = df[['Prompts', 'prompts_with_ontology_demo', 'prompt_with_random_paragraph', 'Annotation']].copy()\n",
    "    output_df.rename(columns={\n",
    "        'prompts_with_ontology_demo': 'prompts_with_ontology_paragraph',\n",
    "        'Prompts': 'prompts_with_random_sentence'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    print(f\"Random paragraph construction completed. Generated {len(output_df)} prompts.\")\n",
    "    return output_df\n",
    "\n",
    "print(\"Random paragraph strategy function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN SENTENCE STRATEGY\n",
    "def construct_domain_sentence_prompts(config):\n",
    "    \"\"\"\n",
    "    Create domain-specific sentence-level examples.\n",
    "    This extends the minimal functionality from Construct_Prompts_with_domain_Sentence.ipynb\n",
    "    \"\"\"\n",
    "    print(\"Starting domain sentence construction...\")\n",
    "    \n",
    "    # Load annotations\n",
    "    df = pd.read_csv(config['input_annotations'])\n",
    "    print(f\"Loaded {len(df)} annotations from {config['input_annotations']}\")\n",
    "    \n",
    "    # Extract entity and relation types\n",
    "    df['entity_types'], df['relation_types'] = zip(*df['Prompts'].apply(extract_entity_relation_types))\n",
    "    \n",
    "    # Extract first sentence (instead of paragraph) from each prompt\n",
    "    df['sentence'] = df['Prompts'].apply(lambda prompt: extract_sentence_from_prompt(prompt, 1) if pd.notna(prompt) else None)\n",
    "    \n",
    "    # Create sentence-based prompts using first sentence as example\n",
    "    df['prompts_with_sentence_demo'] = df.apply(lambda row: \n",
    "        replace_example_in_prompt(row['Prompts'], \n",
    "                                create_example_text(row['sentence'], row['Annotation'])) \n",
    "        if pd.notna(row['sentence']) and pd.notna(row['Annotation']) \n",
    "        else row['Prompts'], axis=1)\n",
    "    \n",
    "    print(f\"Domain sentence construction completed. Generated {len(df)} prompts.\")\n",
    "    return df\n",
    "\n",
    "print(\"Domain sentence strategy function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN EXECUTION ENGINE\n",
    "def run_prompt_construction(config):\n",
    "    \"\"\"Main function to execute the selected prompt construction strategy\"\"\"\n",
    "    \n",
    "    strategy = config['strategy']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXECUTING STRATEGY: {strategy}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Execute the appropriate strategy\n",
    "    if strategy == \"DOMAIN_PARAGRAPH\":\n",
    "        result_df = construct_domain_paragraph_prompts(config)\n",
    "    elif strategy == \"RANDOM_PARAGRAPH\":\n",
    "        result_df = construct_random_paragraph_prompts(config)\n",
    "    elif strategy == \"DOMAIN_SENTENCE\":\n",
    "        result_df = construct_domain_sentence_prompts(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    # Save results\n",
    "    if 'output_csv' in config:\n",
    "        result_df.to_csv(config['output_csv'], index=False)\n",
    "        print(f\"\\nResults saved to: {config['output_csv']}\")\n",
    "    \n",
    "    # Save intermediate results if requested\n",
    "    if config.get('save_intermediate', False):\n",
    "        intermediate_path = config['output_csv'].replace('.csv', '_intermediate.csv')\n",
    "        result_df.to_csv(intermediate_path, index=False)\n",
    "        print(f\"Intermediate results saved to: {intermediate_path}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"Main execution engine loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE PROMPT CONSTRUCTION\n",
    "try:\n",
    "    # Run the configured strategy\n",
    "    result_df = run_prompt_construction(construction_config)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CONSTRUCTION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Strategy: {construction_config['strategy']}\")\n",
    "    print(f\"Total prompts generated: {len(result_df)}\")\n",
    "    print(f\"Output columns: {list(result_df.columns)}\")\n",
    "    \n",
    "    if construction_config.get('debug_output', False):\n",
    "        print(f\"\\nFirst 3 rows preview:\")\n",
    "        print(result_df.head(3))\n",
    "    \n",
    "    print(f\"\\nConstruction completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during construction: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}